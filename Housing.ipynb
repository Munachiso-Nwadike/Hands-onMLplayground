{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "from numpy.core.umath_tests import inner1d\n",
    "from six.moves import urllib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import TransformerMixin #gives fit_transform method for free\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Paths\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = \"datasets/housing\"\n",
    "HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + \"/housing.tgz\"\n",
    "#\n",
    "\n",
    "\n",
    "##Helper functions\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    " if not os.path.isdir(housing_path):\n",
    "  os.makedirs(housing_path)\n",
    " tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    " urllib.request.urlretrieve(housing_url, tgz_path)\n",
    " housing_tgz = tarfile.open(tgz_path)\n",
    " housing_tgz.extractall(path=housing_path)\n",
    " housing_tgz.close()\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    " csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    " return pd.read_csv(csv_path)\n",
    "def split_train_test(data, test_ratio):\n",
    " shuffled_indices = np.random.permutation(len(data))\n",
    " test_set_size = int(len(data) * test_ratio)\n",
    " test_indices = shuffled_indices[:test_set_size]\n",
    " train_indices = shuffled_indices[test_set_size:]\n",
    " return data.iloc[train_indices], data.iloc[test_indices]\n",
    "def test_set_check(identifier, test_ratio, hash):\n",
    " return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratio\n",
    "def split_train_test_by_id(data, test_ratio, id_column, hash=hashlib.md5):\n",
    " ids = data[id_column]\n",
    " in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio, hash))\n",
    " return data.loc[~in_test_set], data.loc[in_test_set]\n",
    "def display_scores(scores):\n",
    " print(\"Scores:\", scores)\n",
    " print(\"Mean:\", scores.mean())\n",
    " print(\"Standard deviation:\", scores.std())\n",
    "##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Combined Attributes Adder\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    " def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "  self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    " def fit(self, X, y=None):\n",
    "  return self # nothing else to do\n",
    " def transform(self, X, y=None):\n",
    "  rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "  population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "  if self.add_bedrooms_per_room:\n",
    "   bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "   return np.c_[X, rooms_per_household, population_per_household,\n",
    "       bedrooms_per_room]\n",
    "  else:\n",
    "   return np.c_[X, rooms_per_household, population_per_household]\n",
    "#\n",
    "#Label Binarizer Class    \n",
    "class MyLabelBinarizer(TransformerMixin):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.encoder = LabelBinarizer(*args, **kwargs)\n",
    "    def fit(self, x, y=0):\n",
    "        self.encoder.fit(x)\n",
    "        return self\n",
    "    def transform(self, x, y=0):\n",
    "        return self.encoder.transform(x) \n",
    "#       \n",
    "#The DataFrameSelector Class\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    " def __init__(self, attribute_names):\n",
    "  self.attribute_names = attribute_names\n",
    " def fit(self, X, y=None):\n",
    "  return self\n",
    " def transform(self, X):\n",
    "  return X[self.attribute_names].values\n",
    "##\n",
    "\n",
    "\n",
    "#Plotting frequencey histograms of each data attribute\n",
    "housing = load_housing_data()\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Splitting the data into training and testing\n",
    "train_set, test_set = split_train_test(housing, 0.2)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Amount to train and test with\")\n",
    "print(len(train_set), \"train +\", len(test_set), \": test\")\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Splitting data into training and test again based on index to avoid generating a different sets each run\n",
    "housing_with_id = housing.reset_index() # adds an `index` column\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Splitting data into training and test based on stratified sampling since our histograms show the incomes aggregate between \n",
    "##two and five and we dont want training data to be biased\n",
    "#rounding up using ceil and merging all the categories greater than 5 into category 5\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "#sample based on income category\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    " strat_train_set = housing.loc[train_index]\n",
    " strat_test_set = housing.loc[test_index]  \n",
    "#print the proportions of income categories to see if sampling did the trick\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Income category proportions\")\n",
    "print(housing[\"income_cat\"].value_counts() / len(housing) )\n",
    "#remove the income_cat attribute so the data is in its original state\n",
    "for set in (strat_train_set, strat_test_set):\n",
    " set.drop([\"income_cat\"], axis=1, inplace=True)\n",
    "##\n",
    "##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Visualising the data further to gain insights\n",
    "#create a copy of the data to play with without harming the training set\n",
    "housing = strat_train_set.copy()\n",
    "#creating a scatterplot of [population?] density over latitute and longitude\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n",
    "#setting the alpha option to 0.1 to more easily visualize high density areas\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n",
    "#visualising housing prices\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    " s=housing[\"population\"]/100, label=\"population\",\n",
    " c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    ")\n",
    "plt.legend()\n",
    "##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Correlations\n",
    "#creating a correlation matrix\n",
    "corr_matrix = housing.corr()\n",
    "#calculating correlation of each attribute with median house value\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"correlation matrix\")\n",
    "print( corr_matrix[\"median_house_value\"].sort_values(ascending=False) )\n",
    "#plotting the correlation visualisations of a bunch of features\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    " \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "#zooming in on median house value since its visualisation appeared most appealing\n",
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
    " alpha=0.1)\n",
    "#creating some new attributes\n",
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
    "#creating another correlation matrix\n",
    "corr_matrix = housing.corr()\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"new correlation matrix\")\n",
    "print( corr_matrix[\"median_house_value\"].sort_values(ascending=False) ) \n",
    "#reverting to a clean training set by copying strat_train_set once again)- drop() creates a copy of the data \n",
    "#and does not affect strat_train_set\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Fixing missing attribute in total_bedroom attribute bu setting values to mediam\n",
    "median = housing[\"total_bedrooms\"].median()\n",
    "housing[\"total_bedrooms\"].fillna(median) \n",
    "imputer = Imputer(strategy=\"median\")\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Imputer Statistics\")\n",
    "print(imputer.statistics_)\n",
    "housing_num.median().values\n",
    "X = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Handling Text using one-hot encoder\n",
    "encoder = LabelEncoder()\n",
    "housing_cat = housing[\"ocean_proximity\"]\n",
    "housing_cat_encoded = encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded \n",
    "encoder = OneHotEncoder()\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
    "housing_cat_1hot\n",
    "housing_cat_1hot.toarray()\n",
    "encoder = LabelBinarizer()\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Creation of Custom Transformers\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "#    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Pipeline Process\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "num_pipeline = Pipeline([\n",
    " ('selector', DataFrameSelector(num_attribs)),\n",
    " ('imputer', Imputer(strategy=\"median\")),\n",
    " ('attribs_adder', CombinedAttributesAdder()),\n",
    " ('std_scaler', StandardScaler()),\n",
    " ])\n",
    "cat_pipeline = Pipeline([\n",
    " ('selector', DataFrameSelector(cat_attribs)),\n",
    " ('label_binarizer', MyLabelBinarizer()),\n",
    " ])\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    " (\"num_pipeline\", num_pipeline),\n",
    " (\"cat_pipeline\", cat_pipeline),\n",
    " ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "housing_prepared\n",
    "housing_prepared.shape\n",
    "##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Training a linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "#Test on some instances from training set\n",
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Linear Regressor Model [No cross val]\")\n",
    "print(\"Predictions:\\t\", lin_reg.predict(some_data_prepared))\n",
    "print(\"Labels:\\t\\t\", list(some_labels))\n",
    "#Measure the regression using RMSE\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(\"Root mean squared error = \", lin_rmse)\n",
    "print(\" \")\n",
    "##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Training a decision tree regressor\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Tree Regressor Model [No cross val] #will give zero error due to overfitting\")\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "#Evaluate on training set\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "print(\"Root mean squared error = \", tree_rmse)\n",
    "print(\" \")\n",
    "##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Training a tree regressor using cross validation feature\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "#Display the score\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Tree regressor scores [Cross Val based]\")\n",
    "display_scores(tree_rmse_scores)\n",
    "##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Training linear regressor with cross validation feature\n",
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
    "                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Linear regressor scores [Cross Val based]\")\n",
    "display_scores(lin_rmse_scores)\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "##Training random forest regressor with cross validation feature\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_scores = cross_val_score( forest_reg, housing_prepared, housing_labels,\n",
    "                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Forest regressor scores [Cross Val based]\")\n",
    "display_scores(forest_rmse_scores)\n",
    "##\n",
    "\n",
    "\n",
    "# #Save the lin_reg, tree_reg and forest_reg models by using sklearn.externals.joblib\n",
    "# from sklearn.externals import joblib\n",
    "# joblib.dump(tree_reg, \"tree_reg.pkl\")\n",
    "# joblib.dump(lin_reg, \"lin_reg.pkl\")\n",
    "# joblib.dump(forest_reg, \"forest_reg.pkl\")\n",
    "# #my_model_loaded = joblib.load(\"my_model.pkl\")\n",
    "# #\n",
    "\n",
    "\n",
    "\n",
    "#Training using grid search to automate multiple cross validations \n",
    "#and pick the hyperperameters that give the best validation\n",
    "#Training RandomForestRegressor with GridSearchCV\n",
    "param_grid = [\n",
    " {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    " {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    " ]\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    " scoring='neg_mean_squared_error')\n",
    "print(grid_search)\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "#Save the best forest_reg model using joblib\n",
    "joblib.dump(forest_reg, \"forest_reg.pkl\")\n",
    "#Listing best parameters\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Best Parameter\")\n",
    "print(grid_search.best_params_)\n",
    "#Listing best estimator \n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Best Estimator\")\n",
    "print(grid_search.best_estimator_)\n",
    "#Printing all parameter values\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"All paramter values and evaluation scores\")\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    " print(np.sqrt(-mean_score), params)\n",
    "#Indicating relative importance of each attribute to making accurate predictions, and importance scores\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_one_hot_attribs = list(encoder.classes_)\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Attribute importances for making accurate predictions\")\n",
    "\n",
    "print( sorted(zip(feature_importances, attributes), reverse=True) )\n",
    "#Evaluating the system on test set\n",
    "final_model = grid_search.best_estimator_\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Forest-reg Grid SearchCV rmse score\")\n",
    "print(\"Score =\", final_rmse)\n",
    "##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###Exercises\n",
    "\n",
    "##Training SVM with kernel=\"linear\" or kernel=\"rbf\" with various values for the C and gamma\n",
    "##hyperparameters \n",
    "param_grid_two = [\n",
    " {'kernel': ('linear', 'rbf' ), 'C': [3, 10, 30], 'gamma': [2, 4, 6, 8]} ]\n",
    "print(\"...\")\n",
    "svm_reg = svm.SVC()\n",
    "print(\"...\")\n",
    "grid_search_two = GridSearchCV(svm_reg, param_grid_two, scoring='neg_mean_squared_error')\n",
    "print(\"...\")\n",
    "print(grid_search_two)\n",
    "grid_search_two.fit(housing_prepared, housing_labels)\n",
    "print(grid_search_two)\n",
    "print(\"...\")\n",
    "#Save the best forest_reg model using joblib\n",
    "print(\"...\")\n",
    "joblib.dump(svm_reg, \"svm_reg.pkl\")\n",
    "print(\"...\")\n",
    "#Evaluating the system on test set\n",
    "print(\"...\")\n",
    "final_model_two = grid_search_two.best_estimator_\n",
    "print(\"...\")\n",
    "X_test_two = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "print(\"...\")\n",
    "y_test_two = strat_test_set[\"median_house_value\"].copy()\n",
    "print(\"...\")\n",
    "X_test_prepared_two = full_pipeline.transform(X_test_two)\n",
    "print(\"...\")\n",
    "final_predictions_two = final_model.predict(X_test_prepared_two)\n",
    "print(\"...\")\n",
    "svm_mse = mean_squared_error(y_test_two, final_predictions_two)\n",
    "print(\"...\")\n",
    "svm_rmse = np.sqrt(svm_mse)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"SVM-based Grid SearchCV rmse score\")\n",
    "print(\"Score =\", svm_rmse)\n",
    "##\n",
    "\n",
    "\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
